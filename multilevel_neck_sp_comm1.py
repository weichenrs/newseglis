# Copyright (c) OpenMMLab. All rights reserved.
import torch.nn as nn
from mmcv.cnn import ConvModule
from mmengine.model.weight_init import xavier_init

from mmseg.registry import MODELS
from ..utils import resize


@MODELS.register_module()
class MultiLevelNeck_sp_comm1(nn.Module):
    """MultiLevelNeck.

    A neck structure connect vit backbone and decoder_heads.

    Args:
        in_channels (List[int]): Number of input channels per scale.
        out_channels (int): Number of output channels (used at each scale).
        scales (List[float]): Scale factors for each input feature map.
            Default: [0.5, 1, 2, 4]
        norm_cfg (dict): Config dict for normalization layer. Default: None.
        act_cfg (dict): Config dict for activation layer in ConvModule.
            Default: None.
    """

    def __init__(self,
                 in_channels,
                 out_channels,
                 scales=[0.5, 1, 2, 4],
                 norm_cfg=None,
                 act_cfg=None):
        super().__init__()
        assert isinstance(in_channels, list)
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.scales = scales
        self.num_outs = len(scales)
        self.lateral_convs = nn.ModuleList()
        self.convs = nn.ModuleList()
        for in_channel in in_channels:
            self.lateral_convs.append(
                ConvModule(
                    in_channel,
                    out_channels,
                    kernel_size=1,
                    norm_cfg=norm_cfg,
                    act_cfg=act_cfg))
        for _ in range(self.num_outs):
            self.convs.append(
                ConvModule(
                    out_channels,
                    out_channels,
                    kernel_size=3,
                    padding=1,
                    stride=1,
                    norm_cfg=norm_cfg,
                    act_cfg=act_cfg))

    # default init_weights for conv(msra) and norm in ConvModule
    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                xavier_init(m, distribution='uniform')

    def forward(self, inputs):

        # import torch.distributed as dist
        # if dist.get_rank() == 0:
        #     import pdb;pdb.set_trace()
        # dist.barrier()    

        # inputs = tuple([my_gather(input) for input in inputs])
        # with torch.no_grad():
        inputs = tuple([_gather_along_last_dim(input) for input in inputs])
        
        assert len(inputs) == len(self.in_channels)
        inputs = [
            lateral_conv(inputs[i])
            for i, lateral_conv in enumerate(self.lateral_convs)
        ]
        # for len(inputs) not equal to self.num_outs
        if len(inputs) == 1:
            inputs = [inputs[0] for _ in range(self.num_outs)]
        outs = []
        for i in range(self.num_outs):
            x_resize = resize(
                inputs[i], scale_factor=self.scales[i], mode='bilinear')
            outs.append(self.convs[i](x_resize))

        # outs = [my_scatter(out) for out in outs]

        return tuple(outs)

import torch
import torch.distributed as dist

# class _GatherFromSequenceParallelRegion(torch.autograd.Function):
#     """Gather the input from sequence parallel region and concatinate."""

#     @staticmethod
#     def symbolic(graph, input_, tensor_parallel_output_grad=True):
#         return _gather_along_first_dim(input_)

#     @staticmethod
#     def forward(ctx, input_, tensor_parallel_output_grad=True):
#         ctx.tensor_parallel_output_grad = tensor_parallel_output_grad
#         return _gather_along_first_dim(input_)

#     @staticmethod
#     def backward(ctx, grad_output):
#         tensor_parallel_output_grad = ctx.tensor_parallel_output_grad

#         # If the computation graph after the gather operation is
#         # in the tensor parallel mode, output gradients need to reduce
#         # scattered and whereas if the computation is duplicated,
#         # output gradients need to be scattered.
#         if tensor_parallel_output_grad:
#             return _reduce_scatter_along_first_dim(grad_output), None
#         else:
#             return _split_along_first_dim(grad_output), None

# class _ReduceScatterToSequenceParallelRegion(torch.autograd.Function):
#     """Reduce scatter the input from the model parallel region."""

#     @staticmethod
#     def symbolic(graph, input_):
#         return _reduce_scatter_along_first_dim(input_)

#     @staticmethod
#     def forward(ctx, input_):
#         return _reduce_scatter_along_first_dim(input_)

#     @staticmethod
#     def backward(ctx, grad_output):
#         return _gather_along_first_dim(grad_output)
    
# def _gather_along_first_dim(input_):
#     """Gather tensors and concatinate along the first dimension."""

#     world_size = dist.get_world_size()
#     # Bypass the function if we are using only 1 GPU.
#     if world_size == 1:
#         return input_

#     dim_size = list(input_.size())
#     dim_size[0] = dim_size[0] * world_size

#     output = torch.empty(dim_size, dtype=input_.dtype, device=torch.cuda.current_device())
#     torch.distributed._all_gather_base(
#         output, input_.contiguous(), group=None()
#     )

#     return output

# def _reduce_scatter_along_first_dim(input_):
#     """Reduce-scatter the input tensor across model parallel group."""
#     world_size = dist.get_world_size()
#     # Bypass the function if we are using only 1 GPU.
#     if world_size == 1:
#         return input_

#     dim_size = list(input_.size())
#     assert (
#         dim_size[0] % world_size == 0
#     ), "First dimension of the tensor should be divisible by tensor parallel size"

#     dim_size[0] = dim_size[0] // world_size

#     output = torch.empty(dim_size, dtype=input_.dtype, device=torch.cuda.current_device())
#     torch.distributed._reduce_scatter_base(
#         output, input_.contiguous(), group=None()
#     )
#     return output

# def _split_along_first_dim(input_):
#     """Split the tensor along its first dimension and keep the
#     corresponding slice."""

#     world_size = dist.get_world_size()
#     # Bypass the function if we are using only 1 GPU.
#     if world_size == 1:
#         return input_

#     # Split along first dimension.
#     dim_size = input_.size()[0]
#     assert (
#         dim_size % world_size == 0
#     ), "First dimension of the tensor should be divisible by tensor parallel size"
#     local_dim_size = dim_size // world_size
#     rank = dist.get_rank()
#     dim_offset = rank * local_dim_size

#     output = input_[dim_offset : dim_offset + local_dim_size].contiguous()

#     return output


def _gather_along_last_dim(input_):
    """Gather tensors and concatinate along the last dimension."""

    world_size = dist.get_world_size()
    # Bypass the function if we are using only 1 GPU.
    if world_size == 1:
        return input_

    # Size and dimension.
    last_dim = input_.dim() - 2
    rank = dist.get_rank()

    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
    tensor_list[rank] = input_
    torch.distributed.all_gather(tensor_list, input_, group=None)

    # Note: torch.cat already creates a contiguous tensor.
    output = torch.cat(tensor_list, dim=last_dim).contiguous()

    return output

def _split_along_last_dim(input_):
    """Split the tensor along its last dimension and keep the
    corresponding slice."""

    world_size = dist.get_world_size()
    # Bypass the function if we are using only 1 GPU.
    if world_size == 1:
        return input_

    # Split along last dimension.
    input_list = split_tensor_along_last_dim(input_, world_size)

    # Note: torch.split does not create contiguous tensors by default.
    rank = dist.get_rank()
    output = input_list[rank].contiguous()

    return output

from typing import List
def split_tensor_along_last_dim(
    tensor: torch.Tensor, num_partitions: int, contiguous_split_chunks: bool = False,
) -> List[torch.Tensor]:
    """ Split a tensor along its last dimension.

        Arguments:
            tensor: input tensor.
            num_partitions: number of partitions to split the tensor
            contiguous_split_chunks: If True, make each chunk contiguous
                                     in memory.

        Returns:
            A list of Tensors
    """
    # Get the size and dimension.
    last_dim = tensor.dim() - 2
    last_dim_size = tensor.size()[last_dim] // num_partitions
    # Split.
    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
    # Note: torch.split does not create contiguous tensors by default.
    if contiguous_split_chunks:
        return tuple(chunk.contiguous() for chunk in tensor_list)

    return tensor_list

class _ScatterToModelParallelRegion(torch.autograd.Function):
    """Split the input and keep only the corresponding chuck to the rank."""

    @staticmethod
    def symbolic(graph, input_):
        return _split_along_last_dim(input_)

    @staticmethod
    def forward(ctx, input_):
        return _split_along_last_dim(input_)

    @staticmethod
    def backward(ctx, grad_output):
        return _gather_along_last_dim(grad_output)

class _GatherFromModelParallelRegion(torch.autograd.Function):
    """Gather the input from model parallel region and concatinate."""

    @staticmethod
    def symbolic(graph, input_):
        return _gather_along_last_dim(input_)

    @staticmethod
    def forward(ctx, input_):
        return _gather_along_last_dim(input_)

    @staticmethod
    def backward(ctx, grad_output):
        return _split_along_last_dim(grad_output)
    
def my_scatter(input_):
    return _ScatterToModelParallelRegion.apply(input_)

def my_gather(input_):
    return _GatherFromModelParallelRegion.apply(input_)